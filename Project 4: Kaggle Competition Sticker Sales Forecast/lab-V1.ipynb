{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the competition and data is available in Kaggle at: https://www.kaggle.com/competitions/playground-series-s5e1/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Sticker 3 Years Sales Forecast Competition\n",
    "\n",
    "end-to-end using Prophet for regression problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger('prophet').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = os.path.join(os.getcwd(),'dataset//playground-series-s5e1.zip')\n",
    "unzip_export_path = os.path.join(os.getcwd(),'dataset')\n",
    "\n",
    "# Open the ZIP file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    # List the contents of the ZIP file\n",
    "    print(\"Contents of the ZIP file:\")\n",
    "    print(zip_ref.namelist())\n",
    "\n",
    "    # Extract all files\n",
    "    zip_ref.extractall(unzip_export_path)\n",
    "    print(\"\\nFiles extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets and Apply Initial Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(os.getcwd(),'dataset//train.csv')\n",
    "test_path = os.path.join(os.getcwd(),'dataset//test.csv')\n",
    "\n",
    "train0 = pd.read_csv(train_path)\n",
    "test0 = pd.read_csv(test_path)\n",
    "\n",
    "display(\n",
    "    train0.tail(),\n",
    "    train0.shape[0],\n",
    "    train0.isna().sum(),\n",
    "    train0.dtypes,\n",
    "    test0.head(),\n",
    "    test0.shape[0],\n",
    "    test0.isna().sum(),\n",
    "    test0.dtypes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fill the NaN fields with 0, and convert date columns into datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train0.copy()\n",
    "test = test0.copy()\n",
    "\n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "train['date'] = pd.to_datetime(train['date'],yearfirst=True)\n",
    "test['date'] = pd.to_datetime(test['date'],yearfirst=True)\n",
    "\n",
    "print(train.dtypes,train.isna().sum())\n",
    "print(test.dtypes,test.isna().sum())\n",
    "\n",
    "display(train.head(),test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand the date range \n",
    "print(train['date'].min(), train['date'].max())\n",
    "print(test['date'].min(),test['date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd  \n",
    "\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def groupby_product(self):\n",
    "        # Creating a 'primary_key' to identify the combination of country, store, and product\n",
    "        self.df['primary_key'] = self.df['country'].astype(str) + '_' + self.df['store'].astype(str) + '_' + self.df['product'].astype(str)\n",
    "        # Grouping by primary_key and date, summing the number of units sold\n",
    "        df_grouped = self.df.groupby(by=['primary_key', 'date'], as_index=False)['num_sold'].sum().reset_index()\n",
    "        df_grouped['num_sold'] = df_grouped['num_sold'].astype(int)  \n",
    "        return df_grouped\n",
    "\n",
    "\n",
    "class EDA_Plot_And_Outlier_Check:\n",
    "    def __init__(self, grouped_df, Z_score):\n",
    "        self.grouped_df = grouped_df\n",
    "        self.Z_score = Z_score\n",
    "\n",
    "    def plot(self):\n",
    "        outlier_dict = {}  # To store outliers for each product\n",
    "        \n",
    "        for product in self.grouped_df['primary_key'].unique():\n",
    "            df1 = self.grouped_df[self.grouped_df['primary_key'] == product]\n",
    "            \n",
    "            # Plot the historical sales data\n",
    "            plt.plot(df1['date'], df1['num_sold'], label='Historical Sales')\n",
    "            plt.title(f'{product} - Historical Sales by Date')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Number of Units Sold')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            # Compute z-score and identify outliers\n",
    "            z_scores = zscore(df1['num_sold'])\n",
    "            outliers = df1[abs(z_scores) > self.Z_score]\n",
    "            outlier_dict[product] = outliers[['date', 'num_sold']] \n",
    "\n",
    "            # # Print outliers for each product\n",
    "            # print(f'Outliers for {product}:')\n",
    "            # print(outliers[['date', 'num_sold']])\n",
    "        \n",
    "        # Convert outlier_dict to a DataFrame after collecting all outliers\n",
    "        outlier_df = pd.concat(outlier_dict.values(), keys=outlier_dict.keys()).reset_index()\n",
    "        outlier_df.rename(columns={'level_0': 'primary_key'}, inplace=True)\n",
    "\n",
    "        return outlier_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessor = DataPreprocessing(train)\n",
    "df_train = Preprocessor.groupby_product()\n",
    "j=0\n",
    "for i in df_train['primary_key'].unique():\n",
    "    if df_train[df_train['primary_key']==i]['num_sold'].sum() == 0:\n",
    "       df_train = df_train[~(df_train['primary_key']==i)]\n",
    "       j+=1\n",
    "print(f'dropped {j} primary keys as there are no sales!')\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_train.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_Plotter = EDA_Plot_And_Outlier_Check(df_train,3)\n",
    "outlier_df = EDA_Plotter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis Result:\n",
    "\n",
    "Based on initial analysis, for all products, it is clear that Christmas period explains most of their outliers. I will now handle few outliers outside of the period for individual product as they seems not to be repetitive events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers to handle: \n",
    "\n",
    "Kerneler Dark Mode - date == 2011-04-24,2011-05-01\n",
    "Kerneler - date == 2011-09-11\n",
    "Kaggle Tiers - date == 2011-11-06\n",
    "\n",
    "Instead of just removal, I will populate them with their previous 7 days sales' average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Outlier_Replace:\n",
    "    def __init__(self, df, prev_days_avg, outlier_df):\n",
    "        self.df = df\n",
    "        self.outlier_df = outlier_df\n",
    "        self.prev_days_avg = prev_days_avg\n",
    "\n",
    "    def replace_avg(self):\n",
    "        for key in self.outlier_df['primary_key'].unique(): \n",
    "            df3 = self.df[self.df['primary_key'] == key]\n",
    "            outlier_df1 = self.outlier_df[self.outlier_df['primary_key'] == key]\n",
    "            \n",
    "            for date in outlier_df1['date']:\n",
    "                # Get the original outlier value\n",
    "                original_outlier_value = df3[df3['date'] == date][\"num_sold\"].values[0]\n",
    "                \n",
    "                # Get previous days up to the given date\n",
    "                previous_days = df3[df3['date'] < date]['date']\n",
    "                df4 = df3[df3['date'].isin(previous_days)]\n",
    "                prev_avg = df4.tail(self.prev_days_avg)['num_sold'].mean()  \n",
    "                \n",
    "                # Check if the previous days' average is NaN, and if so, calculate the next 7 days' average\n",
    "                if pd.isna(prev_avg):\n",
    "                    next_days = df3[df3['date'] > date]['date']\n",
    "                    df5 = df3[df3['date'].isin(next_days)]\n",
    "                    next_avg = df5.head(self.prev_days_avg)['num_sold'].mean()  \n",
    "                    if pd.isna(next_avg):\n",
    "                        next_avg = 0  # Fallback in case both are NaN\n",
    "                    self.df.loc[(self.df['date'] == date) & (self.df['primary_key'] == key), 'num_sold'] = next_avg\n",
    "                    print(f'For {key}, outlier value {original_outlier_value.astype(int)} at {date.date()} has been replaced '\n",
    "                          f'with the next {self.prev_days_avg} days average: {next_avg.astype(int)}')\n",
    "                else:\n",
    "                    # If previous average is valid, use it\n",
    "                    self.df.loc[(self.df['date'] == date) & (self.df['primary_key'] == key), 'num_sold'] = prev_avg\n",
    "                    print(f'For {key}, outlier value {original_outlier_value.astype(int)} at {date.date()} has been replaced '\n",
    "                          f'with the previous {self.prev_days_avg} days average: {prev_avg.astype(int)}')\n",
    "                \n",
    "        return self.df  \n",
    "\n",
    "\n",
    "Outlier_Replace1 = Outlier_Replace(df_train, 7, outlier_df)\n",
    "df_train_OR = Outlier_Replace1.replace_avg()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the plots after the iregular outliers have been handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_Plotter_OR = EDA_Plot_And_Outlier_Check(df_train_OR,3)\n",
    "EDA_Plotter_OR.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am happy about the pattern now to proceed with model training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_OR.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_train_OR.copy()\n",
    "train_data = train_data[['primary_key','date','num_sold']]\n",
    "train_data.rename(columns = {'date':'ds','num_sold':'y'},inplace=True)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORECAST_RESULT = []\n",
    "for product in train_data['primary_key'].unique():\n",
    "    df=train_data[train_data['primary_key']==product].iloc[:,1:]\n",
    "    m = Prophet()\n",
    "    m.fit(df)\n",
    "    future = m.make_future_dataframe(periods=1095)\n",
    "    forecast = m.predict(future)\n",
    "    forecast1= forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].copy()\n",
    "    forecast1['primary_key'] = product\n",
    "    fig2 = m.plot_components(forecast)\n",
    "    fig2.suptitle(f'{product} - Forecast Components Analysis', fontsize=16, y=1.02)\n",
    "    FORECAST_RESULT.append(forecast1)\n",
    "\n",
    "Forecast_Result = pd.concat(FORECAST_RESULT,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotForecastResult:\n",
    "    def __init__(self, forecast, train_data, fig_size=(10, 6)):\n",
    "        \"\"\"\n",
    "        Initializes the class with forecast data, training data, and figure size.\n",
    "\n",
    "        :param forecast: DataFrame containing forecasted results.\n",
    "        :param train_data: DataFrame containing training data.\n",
    "        :param fig_size: Tuple specifying the figure size (width, height).\n",
    "        \"\"\"\n",
    "        self.forecast = forecast\n",
    "        self.train_data = train_data\n",
    "        self.fig_size = fig_size\n",
    "\n",
    "    def plotter(self):\n",
    "        \"\"\"\n",
    "        Plots historical and forecasted data for each product.\n",
    "        \"\"\"\n",
    "        for product in self.forecast['primary_key'].unique():\n",
    "            df1 = self.forecast[self.forecast['primary_key'] == product]\n",
    "            df2 = self.train_data[self.train_data['primary_key'] == product]\n",
    "            \n",
    "            plt.figure(figsize=self.fig_size) \n",
    "            \n",
    "            plt.plot(df1['ds'], df1['yhat'], color='green', label='Forecast')\n",
    "            plt.plot(df2['ds'], df2['y'], color='grey', label='Historical')\n",
    "            plt.title(f'{product} - Historical and Forecasted Sales by Date')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Unit Sales')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPECalculator:\n",
    "    def __init__(self, forecast_result, train_data):\n",
    "        \"\"\"\n",
    "        Initializes the MAPECalculator with forecast results and training data.\n",
    "        \n",
    "        :param forecast_result: DataFrame containing forecasted results.\n",
    "        :param train_data: DataFrame containing training data.\n",
    "        \"\"\"\n",
    "        self.forecast_result = forecast_result\n",
    "        self.train_data = train_data\n",
    "        self.result = None\n",
    "        self.mape_train = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepares the data by merging forecast results with training data \n",
    "        and calculates MAPE for each row.\n",
    "        \"\"\"\n",
    "        self.result = self.forecast_result[['primary_key', 'ds', 'yhat']].merge(\n",
    "            self.train_data, on=['primary_key', 'ds'], how='left'\n",
    "        )\n",
    "        self.result.rename(columns={'ds': 'date', 'yhat': 'forecast', 'y': 'actual'}, inplace=True)\n",
    "        self.result = self.result[['primary_key', 'date', 'actual', 'forecast']]\n",
    "        self.result = self.result[~self.result['actual'].isna()]\n",
    "        self.result['forecast'] = self.result['forecast'].astype(int)\n",
    "        self.result['actual'] = self.result['actual'].astype(int)\n",
    "        self.result['MAPE'] = (abs(self.result['forecast'] - self.result['actual']) / self.result['actual']).round(2)\n",
    "\n",
    "    def calculate_mape(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean MAPE for each product and stores the results.\n",
    "        \"\"\"\n",
    "        mape_train = []\n",
    "        for product in self.result['primary_key'].unique():\n",
    "            result1 = self.result[self.result['primary_key'] == product]\n",
    "            mape = result1['MAPE'].mean().round(2)\n",
    "            mape_train.append({'primary_key': product, 'mape': mape})\n",
    "        self.mape_train = pd.DataFrame(mape_train)\n",
    "\n",
    "    def get_mape(self):\n",
    "        \"\"\"\n",
    "        Returns the DataFrame containing MAPE values for each product.\n",
    "        \"\"\"\n",
    "        return self.mape_train\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Executes the entire MAPE calculation workflow.\n",
    "        \"\"\"\n",
    "        self.prepare_data()\n",
    "        self.calculate_mape()\n",
    "        return self.get_mape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_result = Forecast_Result \n",
    "train_data = train_data \n",
    "\n",
    "mape_calculator = MAPECalculator(forecast_result, train_data)\n",
    "mape = mape_calculator.process()\n",
    "\n",
    "print(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PlotForecastResult:\n",
    "    def __init__(self, forecast, train_data, fig_size=(10, 6)):\n",
    "        \"\"\"\n",
    "        Initializes the class with forecast data, training data, and figure size.\n",
    "\n",
    "        :param forecast: DataFrame containing forecasted results.\n",
    "        :param train_data: DataFrame containing training data.\n",
    "        :param fig_size: Tuple specifying the figure size (width, height).\n",
    "        \"\"\"\n",
    "        self.forecast = forecast\n",
    "        self.train_data = train_data\n",
    "        self.fig_size = fig_size\n",
    "\n",
    "    def plotter(self):\n",
    "        \"\"\"\n",
    "        Plots historical and forecasted data for each product.\n",
    "        Also displays the MAPE (forecast accuracy) for each product.\n",
    "        \"\"\"\n",
    "        for product in self.forecast['primary_key'].unique():\n",
    "            df1 = self.forecast[self.forecast['primary_key'] == product]\n",
    "            df2 = self.train_data[self.train_data['primary_key'] == product]\n",
    "            \n",
    "            # Calculate MAPE for the product\n",
    "            result = df1[['ds', 'yhat']].merge(df2[['ds', 'y']], on='ds', how='left')\n",
    "            result['MAPE'] = (abs(result['yhat'] - result['y']) / result['y']).mean().round(2)\n",
    "\n",
    "            mape_value = result['MAPE'][0]  \n",
    "            \n",
    "            # Create figure with specified size\n",
    "            plt.figure(figsize=self.fig_size)\n",
    "            \n",
    "            # Plot the forecasted and historical data\n",
    "            plt.plot(df1['ds'], df1['yhat'], color='green', label='Forecast')\n",
    "            plt.plot(df2['ds'], df2['y'], color='grey', label='Historical')\n",
    "            \n",
    "            # Add the MAPE in the plot's title\n",
    "            plt.title(f'{product} - Historical and Forecasted Sales by Date\\nMAPE: {mape_value * 100}%')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Unit Sales')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            plt.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = PlotForecastResult(Forecast_Result, train_data, fig_size=(12, 8))\n",
    "plotter.plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission.csv to to complete the Kaggle compitition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_result.tail()\n",
    "final = forecast_result[['primary_key','ds','yhat']]\n",
    "final.rename(columns={'ds':'date','yhat':'forecasted_num_sold'},inplace=True)\n",
    "final['forecasted_num_sold'] = final['forecasted_num_sold'].astype(int)\n",
    "final.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test0\n",
    "test['primary_key'] = test['country'].astype(str)+'_'+test['store'].astype(str)+'_'+test['product'].astype(str)\n",
    "test['date'] = pd.to_datetime(test['date'],yearfirst=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv = final.merge(test,on=['primary_key','date'],how='right')\n",
    "submission_csv['forecasted_num_sold'] = submission_csv['forecasted_num_sold'].fillna(0)\n",
    "submission_csv = submission_csv[['id','forecasted_num_sold']]\n",
    "submission_csv.rename(columns={'forecasted_num_sold':'num_sold'},inplace=True)\n",
    "display(submission_csv.tail())\n",
    "path = os.path.join(os.getcwd(),'submission.csv')\n",
    "submission_csv.to_csv(path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, i will submit it into kaggle competition!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".Kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
